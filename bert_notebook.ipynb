{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12762861-5098-4db9-94e6-e3bfa59c1558",
   "metadata": {},
   "source": [
    "# Bert model\n",
    "\n",
    "The code provided is for building a text classification model using the BERT (Bidirectional Encoder Representations from Transformers) architecture and TensorFlow. The dataset used for classification is loaded from the CSV named 'data.csv' which contains the social media comments. Here is an explanation of what the code does: \n",
    "\n",
    "1. **Imports libraries**: Essential for data manipulation (Pandas, NumPy), deep learning (TensorFlow), NLP (transformers library), and evaluation (scikit-learn and Matplotlib).\n",
    "\n",
    "2. **Loads the data**: Reads the CSV file into a DataFrame, which should contain the text and corresponding labels for the classification task.\n",
    "\n",
    "3. **Initializes BERT tokenizer**: Sets up the tokenizer from the pre-trained 'bert-base-uncased' model to process the text data.\n",
    "\n",
    "4. **Data tokenization function**: The `tokenize_data` function tokenizes the text data into a format suitable for BERT, padding/truncating sequences to a maximum length of 120 tokens.\n",
    "\n",
    "5. **Data splitting**: Splits the dataset into an initial 80% training set and a 20% test set, then tokenizes the test set.\n",
    "\n",
    "6. **K-Fold Cross-Validation Setup**: Sets up 5-fold cross-validation to evaluate model performance during training.\n",
    "\n",
    "7. **Training and Validation Loop**: Trains the model across each fold of the cross-validation, creating separate subsets of the data for training and validation purposes. Training is done in mini-batches of size 16.\n",
    "\n",
    "8. **Model Initialization**: Initializes a new BERT model for sequence classification for each fold, adjusting for the binary classification task (num_labels=2).\n",
    "\n",
    "9. **Optimizer and Loss Function**: Sets up the Adam optimizer and sparse categorical crossentropy loss function for managing multi-class classification in a format that TensorFlow can process. \n",
    "\n",
    "10. **Training with Early Stopping**: Trains the model using an early stopping callback to prevent overfitting, limiting training to a maximum of 3 epochs per fold. \n",
    "\n",
    "11. **Evaluation on Validation Data**: After training on each fold, the model's predictions are evaluated in terms of accuracy, precision, recall, and F1 score. Confusion matrices are also generated to analyze the performance in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc22a1a-8c18-4ea7-8eaf-39df32ac078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x2886b3ce0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x2886b3ce0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "106/106 [==============================] - 425s 4s/step - loss: 0.2829 - accuracy: 0.8889 - val_loss: 0.0965 - val_accuracy: 0.9715\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 472s 4s/step - loss: 0.0773 - accuracy: 0.9780 - val_loss: 0.0881 - val_accuracy: 0.9644\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 918s 9s/step - loss: 0.0337 - accuracy: 0.9905 - val_loss: 0.0779 - val_accuracy: 0.9834\n",
      "27/27 [==============================] - 82s 3s/step\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 918s 9s/step - loss: 0.2852 - accuracy: 0.8984 - val_loss: 0.1204 - val_accuracy: 0.9691\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 654s 6s/step - loss: 0.0643 - accuracy: 0.9816 - val_loss: 0.0998 - val_accuracy: 0.9596\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 592s 6s/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.1021 - val_accuracy: 0.9691\n",
      "27/27 [==============================] - 47s 2s/step\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 756s 7s/step - loss: 0.3053 - accuracy: 0.8723 - val_loss: 0.3155 - val_accuracy: 0.8860\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 416s 4s/step - loss: 0.0971 - accuracy: 0.9703 - val_loss: 0.0811 - val_accuracy: 0.9739\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 439s 4s/step - loss: 0.0462 - accuracy: 0.9863 - val_loss: 0.0877 - val_accuracy: 0.9762\n",
      "27/27 [==============================] - 38s 1s/step\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 470s 4s/step - loss: 0.2936 - accuracy: 0.8806 - val_loss: 0.1279 - val_accuracy: 0.9596\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 457s 4s/step - loss: 0.0827 - accuracy: 0.9739 - val_loss: 0.0655 - val_accuracy: 0.9810\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 459s 4s/step - loss: 0.0321 - accuracy: 0.9917 - val_loss: 0.0727 - val_accuracy: 0.9762\n",
      "27/27 [==============================] - 38s 1s/step\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 476s 4s/step - loss: 0.2915 - accuracy: 0.8901 - val_loss: 0.1193 - val_accuracy: 0.9667\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 462s 4s/step - loss: 0.0942 - accuracy: 0.9745 - val_loss: 0.1076 - val_accuracy: 0.9619\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 465s 4s/step - loss: 0.0381 - accuracy: 0.9917 - val_loss: 0.1063 - val_accuracy: 0.9643\n",
      "27/27 [==============================] - 39s 1s/step\n",
      "Accuracy scores for each fold: [0.9833729216152018, 0.9596199524940617, 0.9738717339667459, 0.9809976247030879, 0.9642857142857143]\n",
      "Precision scores for each fold: [0.9752475247524752, 0.958139534883721, 0.966183574879227, 0.9812206572769953, 0.9575471698113207]\n",
      "Recall scores for each fold: [0.9899497487437185, 0.9626168224299065, 0.9803921568627451, 0.9812206572769953, 0.9712918660287081]\n",
      "F1 scores for each fold: [0.9825436408977556, 0.9603729603729604, 0.9732360097323601, 0.9812206572769953, 0.9643705463182898]\n",
      "Confusion matrices for each fold:\n",
      "[array([[217,   5],\n",
      "       [  2, 197]]), array([[198,   9],\n",
      "       [  8, 206]]), array([[210,   7],\n",
      "       [  4, 200]]), array([[204,   4],\n",
      "       [  4, 209]]), array([[202,   9],\n",
      "       [  6, 203]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "132/132 [==============================] - 554s 4s/step - loss: 0.1946 - accuracy: 0.9326\n",
      "Epoch 2/3\n",
      "132/132 [==============================] - 539s 4s/step - loss: 0.0301 - accuracy: 0.9914\n",
      "Epoch 3/3\n",
      "132/132 [==============================] - 534s 4s/step - loss: 0.0091 - accuracy: 0.9976\n",
      "27/27 [==============================] - 37s 1s/step\n",
      "Final model accuracy: 1.0\n",
      "Final model precision: 1.0\n",
      "Final model recall: 1.0\n",
      "Final model F1 score: 1.0\n",
      "Final model confusion matrix:\n",
      "[[218   0]\n",
      " [  0 203]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('data/all_social_media_posts.csv')\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the data\n",
    "def tokenize_data(texts, max_length=120):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Split the data into initial training and test sets (80% train and 20% test)\n",
    "initial_train_texts, initial_test_texts, initial_train_labels, initial_test_labels = train_test_split(\n",
    "    df['Content'].tolist(),\n",
    "    df['Eating_Disorder'].tolist(),\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Tokenize the initial test set\n",
    "initial_test_encodings = tokenize_data(initial_test_texts)\n",
    "\n",
    "# Setting up KFold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Lists to hold scores\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_ids, validation_ids) in enumerate(kfold.split(df)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    \n",
    "    # Prepare the datasets for training and validation\n",
    "    train_encodings = tokenize_data(df.iloc[train_ids]['Content'].tolist())\n",
    "    validation_encodings = tokenize_data(df.iloc[validation_ids]['Content'].tolist())\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(input_ids=train_encodings['input_ids'], attention_mask=train_encodings['attention_mask']),\n",
    "        df.iloc[train_ids]['Eating_Disorder'].tolist()\n",
    "    )).shuffle(10000).batch(16)\n",
    "    \n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(input_ids=validation_encodings['input_ids'], attention_mask=validation_encodings['attention_mask']),\n",
    "        df.iloc[validation_ids]['Eating_Disorder'].tolist()\n",
    "    )).batch(16)\n",
    "    \n",
    "    # Initialize the model for each fold\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    \n",
    "    # Set up the legacy TF-Keras Adam optimizer for M1/M2 Macs\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model on the current fold's data\n",
    "    model.fit(train_dataset, validation_data=validation_dataset, epochs=3, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model after each fold if necessary\n",
    "    # fold_model_save_path = f'model_save_path_fold_{fold+1}'\n",
    "    # model.save_pretrained(fold_model_save_path)\n",
    "    \n",
    "    # Predict on the validation set and calculate metrics\n",
    "    logits = model.predict(validation_dataset).logits\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Collect true labels for the current fold\n",
    "    true_labels = df.iloc[validation_ids]['Eating_Disorder'].tolist()\n",
    "    \n",
    "    # Calculate and append metrics for the current fold\n",
    "    accuracy_scores.append(accuracy_score(true_labels, predictions))\n",
    "    precision_scores.append(precision_score(true_labels, predictions))\n",
    "    recall_scores.append(recall_score(true_labels, predictions))\n",
    "    f1_scores.append(f1_score(true_labels, predictions))\n",
    "    confusion_matrices.append(confusion_matrix(true_labels, predictions))\n",
    "\n",
    "# Output the scores and confusion matrices\n",
    "print(f'Accuracy scores for each fold: {accuracy_scores}')\n",
    "print(f'Precision scores for each fold: {precision_scores}')\n",
    "print(f'Recall scores for each fold: {recall_scores}')\n",
    "print(f'F1 scores for each fold: {f1_scores}')\n",
    "print(f'Confusion matrices for each fold:\\n{confusion_matrices}')\n",
    "\n",
    "# Train final model on the entire dataset\n",
    "final_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "final_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "full_encodings = tokenize_data(df['Content'].tolist())\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(input_ids=full_encodings['input_ids'], attention_mask=full_encodings['attention_mask']),\n",
    "    df['Eating_Disorder'].tolist()\n",
    ")).shuffle(10000).batch(16)\n",
    "\n",
    "final_model.fit(full_dataset, epochs=3)\n",
    "\n",
    "# Evaluate the final model on the initial test set\n",
    "initial_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': initial_test_encodings['input_ids'], 'attention_mask': initial_test_encodings['attention_mask']},\n",
    "    initial_test_labels\n",
    ")).batch(16)\n",
    "\n",
    "logits = final_model.predict(initial_test_dataset).logits\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Calculate metrics for the final model\n",
    "accuracy = accuracy_score(initial_test_labels, predictions)\n",
    "precision = precision_score(initial_test_labels, predictions)\n",
    "recall = recall_score(initial_test_labels, predictions)\n",
    "f1 = f1_score(initial_test_labels, predictions)\n",
    "confusion_mat = confusion_matrix(initial_test_labels, predictions)\n",
    "\n",
    "# Output the scores and confusion matrix for the final model\n",
    "print(f'Final model accuracy: {accuracy}')\n",
    "print(f'Final model precision: {precision}')\n",
    "print(f'Final model recall: {recall}')\n",
    "print(f'Final model F1 score: {f1}')\n",
    "print(f'Final model confusion matrix:\\n{confusion_mat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dce7c6-c1d5-4b44-8e66-994dd75e11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model with the name \"final_bert\"\n",
    "final_model.save_pretrained('final_bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87073-d796-46fc-9b55-57eb8af346e5",
   "metadata": {},
   "source": [
    "The results are exceptionally good and even indicate perfection in terms of accuracy, precision, recall, and F1-score on the test dataset. \n",
    "\n",
    "This could maybe stem from the fact that the problem the model is intended to solve could be too simple. Maybe the reason for that is that the synthetic training data has not really much ambiguity in them. Therefore, patterns of the two classes might have been very easy to learn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
